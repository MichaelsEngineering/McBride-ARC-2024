{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-26T00:42:06.565232Z","iopub.status.busy":"2024-10-26T00:42:06.564917Z","iopub.status.idle":"2024-10-26T00:42:24.514876Z","shell.execute_reply":"2024-10-26T00:42:24.513535Z","shell.execute_reply.started":"2024-10-26T00:42:06.565197Z"},"trusted":true},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#By Iffy_ https://www.kaggle.com/code/irfanmansuri/lm-llama3b-instruct/notebook\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\nimport torch\nfrom accelerate import infer_auto_device_map\nimport gc\nfrom IPython.display import Markdown, display\n\n#Boolean Face\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\nfrom typing import List, Dict, Tuple, Optional, Set\nfrom collections import defaultdict\nimport itertools\n\n\n# Option 1 (Recommended for T4 x2): Enable tokenizer parallelism\n# T4s have enough memory and processing power to benefit from parallel tokenization\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\n# Enable CUDA memory optimizations\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n# Clear GPU memory before loading model\ngc.collect()\ntorch.cuda.empty_cache()"},{"cell_type":"markdown","metadata":{},"source":"### Transformers"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:24.519209Z","iopub.status.busy":"2024-10-26T00:42:24.516503Z","iopub.status.idle":"2024-10-26T00:42:24.524842Z","shell.execute_reply":"2024-10-26T00:42:24.52237Z","shell.execute_reply.started":"2024-10-26T00:42:24.519171Z"},"trusted":true},"outputs":[],"source":"# %%capture\n# %pip install -U transformers accelerate"},{"cell_type":"markdown","metadata":{},"source":"### Load Data"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:24.526637Z","iopub.status.busy":"2024-10-26T00:42:24.526291Z","iopub.status.idle":"2024-10-26T00:42:24.547928Z","shell.execute_reply":"2024-10-26T00:42:24.546924Z","shell.execute_reply.started":"2024-10-26T00:42:24.526596Z"},"trusted":true},"outputs":[],"source":"practice_path = '/kaggle/input/practice/practice_training_challenge.json'\n\ndef solve_arc_task(task):\n    \"\"\"\n    Solve an ARC task and return predictions for all test inputs.\n    \"\"\"\n    train_data = task[\"train\"]\n    test_inputs = task[\"test\"][\"input\"]\n    \n    # Handle both single test input and multiple test inputs\n    if not isinstance(test_inputs[0][0], list):\n        test_inputs = [test_inputs]  # Convert single test input to list\n    \n    # Create predictions for each test input\n    predictions = []\n    for test_input in test_inputs:\n        # Use training data to determine pattern\n        train_grids = [np.array(example[\"input\"]) for example in train_data]\n        output_grid = np.bitwise_or.reduce(train_grids)\n        \n        # Solve for current test input\n        test_output = np.bitwise_or(output_grid, np.array(test_input))\n        predictions.append(test_output.tolist())\n    \n    return predictions\n\ndef format_submission(task_results):\n    \"\"\"\n    Format results according to competition submission requirements.\n    Handles multiple test cases per task.\n    \"\"\"\n    submission = {}\n    \n    for task_id, predictions in task_results.items():\n        # Create list of predictions for each test case\n        submission[task_id] = [\n            {\n                \"attempt_1\": pred,\n                \"attempt_2\": pred  \n            }\n            for pred in predictions\n        ]\n    \n    return submission\n\n# Load the challenge data from file\nwith open(practice_path, 'r') as f:\n    challenge_tasks = json.load(f)\n\n# Process tasks and create predictions\ntask_results = {}\nfor task_id, task_data in challenge_tasks.items():\n    task_results[task_id] = solve_arc_task(task_data)\n\n# Format the results in the required submission format\nsubmission = format_submission(task_results)\n\n# Save submission to JSON file\nwith open('submission.json', 'w') as f:\n    json.dump(submission, f, indent=2)\n\n# Print the formatted submission\nprint(\"Submission format:\")\nprint(json.dumps(submission, indent=2))"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:24.55108Z","iopub.status.busy":"2024-10-26T00:42:24.550716Z","iopub.status.idle":"2024-10-26T00:42:55.318027Z","shell.execute_reply":"2024-10-26T00:42:55.317255Z","shell.execute_reply.started":"2024-10-26T00:42:24.55104Z"},"trusted":true},"outputs":[],"source":"base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n\n\n# Load tokenizer with caching\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    use_fast=True,  # Fast tokenizer is crucial for T4 performance\n    cache_dir=\"./cache\",\n    padding_side=\"left\",\n    truncation=True,\n    use_threading=True  # Enable threading for parallel processing\n)\n\n# Configure model for dual T4s\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    torch_dtype=torch.float16,  # FP16 for optimal T4 performance\n    device_map=\"auto\",  # Let accelerate handle dual GPU distribution\n    low_cpu_mem_usage=True,\n    use_cache=True,\n    max_memory={\n        0: \"11GiB\",  # Reserve some memory for CUDA overhead\n        1: \"11GiB\",  # T4s have 16GB each, leaving buffer\n        \"cpu\": \"24GiB\"  # Generous CPU memory for caching\n    },\n    offload_folder=\"offload\",\n    trust_remote_code=True\n)"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:55.319434Z","iopub.status.busy":"2024-10-26T00:42:55.31913Z","iopub.status.idle":"2024-10-26T00:42:55.324394Z","shell.execute_reply":"2024-10-26T00:42:55.3234Z","shell.execute_reply.started":"2024-10-26T00:42:55.319403Z"},"trusted":true},"outputs":[],"source":"#By Iffy_ https://www.kaggle.com/code/irfanmansuri/lm-llama3b-instruct/notebook\n\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\nif model.config.pad_token_id is None:\n    model.config.pad_token_id = model.config.eos_token_id"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:55.326833Z","iopub.status.busy":"2024-10-26T00:42:55.326059Z","iopub.status.idle":"2024-10-26T00:42:55.387136Z","shell.execute_reply":"2024-10-26T00:42:55.386406Z","shell.execute_reply.started":"2024-10-26T00:42:55.326755Z"},"trusted":true},"outputs":[],"source":"#By Iffy_ https://www.kaggle.com/code/irfanmansuri/lm-llama3b-instruct/notebook\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    batch_size=2,  # Optimal for dual T4s with this model size\n    max_length=2048\n)"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:55.388448Z","iopub.status.busy":"2024-10-26T00:42:55.388193Z","iopub.status.idle":"2024-10-26T00:43:09.068672Z","shell.execute_reply":"2024-10-26T00:43:09.067692Z","shell.execute_reply.started":"2024-10-26T00:42:55.388419Z"},"trusted":true},"outputs":[],"source":"#By Iffy_ https://www.kaggle.com/code/irfanmansuri/lm-llama3b-instruct/notebook\n\nfrom IPython.display import Markdown, display\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are an https://arcprize.org/guide expert\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Who is FranÃƒÆ’Ã‚Â§ois Chollet?\",\n    },\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n\noutputs = pipe(prompt,truncation=True, do_sample=True)\n\ndisplay(\n    Markdown(\n            outputs[0][\"generated_text\"].split(\n                \"<|start_header_id|>assistant<|end_header_id|>\"\n            )[1]\n        )\n    )"},{"cell_type":"markdown","metadata":{},"source":"### Boolean Face"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:43:09.070741Z","iopub.status.busy":"2024-10-26T00:43:09.070336Z","iopub.status.idle":"2024-10-26T00:43:09.10683Z","shell.execute_reply":"2024-10-26T00:43:09.105832Z","shell.execute_reply.started":"2024-10-26T00:43:09.070698Z"},"trusted":true},"outputs":[],"source":"def boolean_solver(input_data):\n    # Convert JSON input to numpy array for easier processing\n    grids = [np.array(grid) for grid in input_data]\n    \n    # We assume the operation is \"logical OR\" across all grid inputs\n    output_grid = np.bitwise_or.reduce(grids)\n    \n    return output_grid.tolist()\n\n\nclass BooleanOperator(Enum):\n    AND = \"and\"\n    OR = \"or\"\n    XOR = \"xor\"\n    NAND = \"nand\"\n    NOR = \"nor\"\n    NOT = \"not\"\n    XNOR = \"xnor\"\n    \n@dataclass\nclass TruthTableEntry:\n    input_states: Tuple[np.ndarray, ...]\n    output_state: np.ndarray\n    operator: BooleanOperator\n    score: float\n    description: str = \"\"\n\nclass TruthTableAnalyzer:\n    \"\"\"Analyzes 2x2 matrix patterns using truth tables and cellular automata principles\"\"\"\n    \n    def __init__(self, logging_level=logging.INFO):\n        self.setup_logging(logging_level)\n        self.truth_table_cache = {}\n        self.score_history = defaultdict(list)\n        self.best_rules = {}\n        \n    def setup_logging(self, level):\n        \"\"\"Configure detailed logging for analysis\"\"\"\n        logging.basicConfig(\n            level=level,\n            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('boolean_analysis.log'),\n                logging.StreamHandler()\n            ]\n        )\n        self.logger = logging.getLogger('BooleanAnalyzer')\n        \n    def analyze_2x2_pattern(self, input_grid: np.ndarray, output_grid: np.ndarray) -> TruthTableEntry:\n        \"\"\"Analyze a 2x2 pattern and generate truth table\"\"\"\n        self.logger.info(f\"\\nAnalyzing 2x2 pattern:\\nInput:\\n{input_grid}\\nOutput:\\n{output_grid}\")\n        \n        # Convert to boolean\n        input_bool = input_grid.astype(bool)\n        output_bool = output_grid.astype(bool)\n        \n        # Generate cache key\n        cache_key = self._generate_cache_key(input_bool, output_bool)\n        \n        if cache_key in self.truth_table_cache:\n            self.logger.info(\"Found pattern in cache\")\n            return self.truth_table_cache[cache_key]\n        \n        # Try all boolean operators and score results\n        best_entry = self._find_best_operator(input_bool, output_bool)\n        \n        # Cache result\n        self.truth_table_cache[cache_key] = best_entry\n        \n        return best_entry\n    \n    def _generate_cache_key(self, input_bool: np.ndarray, output_bool: np.ndarray) -> str:\n        \"\"\"Generate unique key for caching\"\"\"\n        return f\"{input_bool.tobytes()}-{output_bool.tobytes()}\"\n    \n    def _find_best_operator(self, input_bool: np.ndarray, output_bool: np.ndarray) -> TruthTableEntry:\n        \"\"\"Try different boolean operators and find best match\"\"\"\n        best_score = 0.0\n        best_entry = None\n        \n        for operator in BooleanOperator:\n            result = self._apply_operator(input_bool, operator)\n            score = self._calculate_match_score(result, output_bool)\n            \n            self.logger.debug(f\"Operator {operator.value}:\")\n            self.logger.debug(f\"Result:\\n{result}\")\n            self.logger.debug(f\"Score: {score}\")\n            \n            if score > best_score:\n                best_score = score\n                best_entry = TruthTableEntry(\n                    input_states=(input_bool,),\n                    output_state=result,\n                    operator=operator,\n                    score=score\n                )\n                \n        self.logger.info(f\"Best operator: {best_entry.operator.value} with score {best_entry.score}\")\n        return best_entry\n    \n    def _apply_operator(self, grid: np.ndarray, operator: BooleanOperator) -> np.ndarray:\n        \"\"\"Apply boolean operator to grid\"\"\"\n        if operator == BooleanOperator.NOT:\n            return ~grid\n        elif operator == BooleanOperator.AND:\n            return grid & np.roll(grid, 1, axis=0)\n        elif operator == BooleanOperator.OR:\n            return grid | np.roll(grid, 1, axis=0)\n        elif operator == BooleanOperator.XOR:\n            return grid ^ np.roll(grid, 1, axis=0)\n        elif operator == BooleanOperator.NAND:\n            return ~(grid & np.roll(grid, 1, axis=0))\n        elif operator == BooleanOperator.NOR:\n            return ~(grid | np.roll(grid, 1, axis=0))\n        elif operator == BooleanOperator.XNOR:\n            return ~(grid ^ np.roll(grid, 1, axis=0))\n            \n    def _calculate_match_score(self, result: np.ndarray, target: np.ndarray) -> float:\n        \"\"\"Calculate match score between result and target\"\"\"\n        return np.mean(result == target)\n\nclass BooleanAlgebraFace:\n    \"\"\"Main Boolean Algebra face implementation\"\"\"\n    \n    def __init__(self, llm_pipeline=None):\n        self.analyzer = TruthTableAnalyzer()\n        self.llm_pipeline = llm_pipeline\n        self.learning_rate = 0.1\n        self.min_confidence = 0.6\n        \n    def analyze_task(self, task: Dict) -> Tuple[List[List[int]], float]:\n        \"\"\"Analyze task and return prediction with confidence score\"\"\"\n        input_grids = [np.array(pair['input']) for pair in task['train']]\n        output_grids = [np.array(pair['output']) for pair in task['train']]\n        \n        # Analyze patterns starting with 2x2 windows\n        patterns = self._extract_2x2_patterns(input_grids[0], output_grids[0])\n        \n        # Get truth table analysis for each pattern\n        truth_tables = []\n        for in_pattern, out_pattern in patterns:\n            entry = self.analyzer.analyze_2x2_pattern(in_pattern, out_pattern)\n            truth_tables.append(entry)\n            \n        # Generate description using LLM if available\n        if self.llm_pipeline and any(entry.score > self.min_confidence for entry in truth_tables):\n            best_entry = max(truth_tables, key=lambda x: x.score)\n            description = self._generate_llm_description(best_entry)\n            best_entry.description = description\n            \n        # Apply learned patterns to test input\n        test_input = np.array(task['test']['input'])\n        prediction, confidence = self._apply_patterns(test_input, truth_tables)\n        \n        return prediction.tolist(), confidence\n    \n    def _extract_2x2_patterns(self, input_grid: np.ndarray, output_grid: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Extract all 2x2 patterns from input/output grids\"\"\"\n        patterns = []\n        rows, cols = input_grid.shape\n        \n        for i in range(rows-1):\n            for j in range(cols-1):\n                in_pattern = input_grid[i:i+2, j:j+2]\n                out_pattern = output_grid[i:i+2, j:j+2]\n                patterns.append((in_pattern, out_pattern))\n                \n        return patterns\n    \n    def _generate_llm_description(self, entry: TruthTableEntry) -> str:\n        \"\"\"Generate natural language description of boolean pattern\"\"\"\n        prompt = f\"\"\"\n        Describe this boolean pattern transformation:\n        Input matrix:\n        {entry.input_states[0]}\n        \n        Operation applied: {entry.operator.value}\n        \n        Output matrix:\n        {entry.output_state}\n        \n        Score: {entry.score}\n        \n        Describe the pattern in natural language:\n        \"\"\"\n        \n        response = self.llm_pipeline(prompt, max_length=100)[0]['generated_text']\n        return response.strip()\n    \n    def _apply_patterns(self, test_input: np.ndarray, truth_tables: List[TruthTableEntry]) -> Tuple[np.ndarray, float]:\n        \"\"\"Apply learned patterns to test input\"\"\"\n        # Start with highest scoring patterns\n        truth_tables.sort(key=lambda x: x.score, reverse=True)\n        \n        # Initialize output grid\n        output = np.zeros_like(test_input)\n        total_confidence = 0.0\n        \n        # Apply each pattern where confidence is high enough\n        for entry in truth_tables:\n            if entry.score > self.min_confidence:\n                transformed = self._apply_operator(test_input, entry.operator)\n                output = np.where(entry.score > total_confidence, transformed, output)\n                total_confidence = max(total_confidence, entry.score)\n                \n        return output, total_confidence\n\n    def update_learning(self, task_id: str, prediction: List[List[int]], actual: List[List[int]], score: float):\n        \"\"\"Update learning based on feedback\"\"\"\n        self.analyzer.score_history[task_id].append(score)\n        \n        # Adjust learning rate based on historical performance\n        if len(self.analyzer.score_history[task_id]) > 1:\n            prev_score = self.analyzer.score_history[task_id][-2]\n            if score > prev_score:\n                self.learning_rate *= 1.1\n            else:\n                self.learning_rate *= 0.9\n                \n        self.analyzer.logger.info(f\"Task {task_id} - Score: {score}, Learning rate: {self.learning_rate}\")"},{"cell_type":"markdown","metadata":{},"source":"### Usage"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:45:43.06936Z","iopub.status.busy":"2024-10-26T00:45:43.068717Z","iopub.status.idle":"2024-10-26T00:45:43.085135Z","shell.execute_reply":"2024-10-26T00:45:43.084018Z","shell.execute_reply.started":"2024-10-26T00:45:43.069317Z"},"trusted":true},"outputs":[],"source":"class ARCProcessor:\n    def __init__(self, boolean_face=None, llm_pipeline=None):\n        self.boolean_face = boolean_face or BooleanAlgebraFace(llm_pipeline)\n        \n    def process_tasks(self, tasks: Dict) -> Dict:\n        results = {}\n        for task_id, task_data in tasks.items():\n            print(f\"Processing task {task_id}\")\n            predictions = self.process_single_task(task_data)\n            results[task_id] = predictions\n        return results\n    \n    def process_single_task(self, task_data: Dict) -> List[Dict]:\n        # Handle both single and multiple test inputs\n        test_inputs = task_data.get('test', {})\n        if isinstance(test_inputs, dict):\n            test_inputs = [test_inputs]\n            \n        predictions = []\n        for test_input in test_inputs:\n            task = {\n                'train': task_data['train'],\n                'test': test_input\n            }\n            pred_grid, confidence = self.boolean_face.analyze_task(task)\n            \n            # Create two slightly different attempts\n            attempt_1 = pred_grid\n            attempt_2 = self._create_alternative_attempt(pred_grid)\n            \n            prediction = {\n                'attempt_1': attempt_1,\n                'attempt_2': attempt_2\n            }\n            predictions.append(prediction)\n            \n        return predictions\n    \n    def _create_alternative_attempt(self, pred_grid: List[List[int]]) -> List[List[int]]:\n        \"\"\"Create a slightly modified version of the prediction for the second attempt\"\"\"\n        grid = np.array(pred_grid)\n        if grid.size > 0:\n            # Randomly modify one cell for variety\n            i, j = np.random.randint(0, grid.shape[0]), np.random.randint(0, grid.shape[1])\n            grid[i, j] = 1 - grid[i, j]  # Flip 0 to 1 or 1 to 0\n        return grid.tolist()\n    \n    def save_submission(self, results: Dict, output_path: str = 'submission.json'):\n        with open(output_path, 'w') as f:\n            json.dump(results, f, indent=2)\n            \ndef run_arc_processor(test_file_path: str, llm_pipeline=None):\n    # Load tasks\n    with open(test_file_path, 'r') as f:\n        tasks = json.load(f)\n    \n    # Initialize components\n    print(\"Initializing Boolean Algebra face...\")\n    boolean_face = BooleanAlgebraFace(llm_pipeline)\n    \n    # Create processor\n    print(\"Creating processor...\")\n    processor = ARCProcessor(boolean_face, llm_pipeline)\n    \n    # Process tasks\n    print(\"Processing tasks...\")\n    results = processor.process_tasks(tasks)\n    \n    # Save submission\n    processor.save_submission(results)\n    return results"},{"cell_type":"markdown","metadata":{},"source":"### Acknowledgements:\n\nIffy_ https://www.kaggle.com/code/irfanmansuri/lm-llama3b-instruct/notebook"}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8951125,"sourceId":67357,"sourceType":"competition"},{"datasetId":5943062,"sourceId":9714901,"sourceType":"datasetVersion"},{"modelId":121027,"modelInstanceId":100936,"sourceId":120005,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}