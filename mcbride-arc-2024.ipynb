{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-26T00:42:06.565232Z","iopub.status.busy":"2024-10-26T00:42:06.564917Z","iopub.status.idle":"2024-10-26T00:42:24.514876Z","shell.execute_reply":"2024-10-26T00:42:24.513535Z","shell.execute_reply.started":"2024-10-26T00:42:06.565197Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'transformers'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, filename))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#By Iffy_ https://www.kaggle.com/code/irfanmansuri/lm-llama3b-instruct/notebook\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_auto_device_map\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","import json\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n","import torch\n","from accelerate import infer_auto_device_map\n","import gc\n","from IPython.display import Markdown, display\n","\n","#Boolean Face\n","from dataclasses import dataclass\n","from enum import Enum\n","import logging\n","from typing import List, Dict, Tuple, Optional, Set\n","from collections import defaultdict\n","import itertools\n","\n","# Set up logger\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","from src.utils.utils import load_and_log_first_task\n","\n","\n","# Option 1 (Recommended for T4 x2): Enable tokenizer parallelism\n","# T4s have enough memory and processing power to benefit from parallel tokenization\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n","\n","# Enable CUDA memory optimizations\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","# Clear GPU memory before loading model\n","gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:24.519209Z","iopub.status.busy":"2024-10-26T00:42:24.516503Z","iopub.status.idle":"2024-10-26T00:42:24.524842Z","shell.execute_reply":"2024-10-26T00:42:24.52237Z","shell.execute_reply.started":"2024-10-26T00:42:24.519171Z"},"trusted":true},"outputs":[],"source":["# %%capture\n","# %pip install -U transformers accelerate"]},{"cell_type":"markdown","metadata":{},"source":["### Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:24.526637Z","iopub.status.busy":"2024-10-26T00:42:24.526291Z","iopub.status.idle":"2024-10-26T00:42:24.547928Z","shell.execute_reply":"2024-10-26T00:42:24.546924Z","shell.execute_reply.started":"2024-10-26T00:42:24.526596Z"},"trusted":true},"outputs":[],"source":["# practice_path = '/kaggle/input/practice/practice_training_challenge.json'\n","kaggle_path = 'kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n","\n","# Load challenges and log the first task\n","challenge_tasks = load_and_log_first_task(kaggle_path)\n","\n","if challenge_tasks:\n","    # Get the first task ID and data\n","        # Load all task IDs into an array\n","    task_ids = list(challenge_tasks.keys())\n","    first_task_id = next(iter(challenge_tasks))\n","    first_task_data = challenge_tasks[first_task_id]\n","\n","    # Now you can work with the first task\n","    print(f\"Working with task ID: {first_task_id}\")\n","\n","    # If you want to process all tasks, you can still do so:\n","    # for task_id, task_data in challenge_tasks.items():\n","    #     result = solve_arc_task(task_data)\n","    #     # Process or store the result as needed\n","else:\n","    print(\"Failed to load challenge tasks.\")"]},{"cell_type":"markdown","metadata":{},"source":["### Setup up llama-3.2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:24.55108Z","iopub.status.busy":"2024-10-26T00:42:24.550716Z","iopub.status.idle":"2024-10-26T00:42:55.318027Z","shell.execute_reply":"2024-10-26T00:42:55.317255Z","shell.execute_reply.started":"2024-10-26T00:42:24.55104Z"},"trusted":true},"outputs":[],"source":["base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n","\n","\n","# Load tokenizer with caching\n","tokenizer = AutoTokenizer.from_pretrained(\n","    base_model,\n","    use_fast=True,  # Fast tokenizer is crucial for T4 performance\n","    cache_dir=\"./cache\",\n","    padding_side=\"left\",\n","    truncation=True,\n","    use_threading=True  # Enable threading for parallel processing\n",")\n","\n","# Configure model for dual T4s\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    torch_dtype=torch.float16,  # FP16 for optimal T4 performance\n","    device_map=\"auto\",  # Let accelerate handle dual GPU distribution\n","    low_cpu_mem_usage=True,\n","    use_cache=True,\n","    max_memory={\n","        0: \"11GiB\",  # Reserve some memory for CUDA overhead\n","        1: \"11GiB\",  # T4s have 16GB each, leaving buffer\n","        \"cpu\": \"24GiB\"  # Generous CPU memory for caching\n","    },\n","    offload_folder=\"offload\",\n","    trust_remote_code=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:55.319434Z","iopub.status.busy":"2024-10-26T00:42:55.31913Z","iopub.status.idle":"2024-10-26T00:42:55.324394Z","shell.execute_reply":"2024-10-26T00:42:55.3234Z","shell.execute_reply.started":"2024-10-26T00:42:55.319403Z"},"trusted":true},"outputs":[],"source":["if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","if model.config.pad_token_id is None:\n","    model.config.pad_token_id = model.config.eos_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:55.326833Z","iopub.status.busy":"2024-10-26T00:42:55.326059Z","iopub.status.idle":"2024-10-26T00:42:55.387136Z","shell.execute_reply":"2024-10-26T00:42:55.386406Z","shell.execute_reply.started":"2024-10-26T00:42:55.326755Z"},"trusted":true},"outputs":[],"source":["\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    batch_size=2,  # Optimal for dual T4s with this model size\n","    max_length=2048\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:42:55.388448Z","iopub.status.busy":"2024-10-26T00:42:55.388193Z","iopub.status.idle":"2024-10-26T00:43:09.068672Z","shell.execute_reply":"2024-10-26T00:43:09.067692Z","shell.execute_reply.started":"2024-10-26T00:42:55.388419Z"},"trusted":true},"outputs":[],"source":["\n","from IPython.display import Markdown, display\n","\n","messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You are an https://arcprize.org/guide expert\",\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"Who is Francois Chollet?\",\n","    },\n","]\n","\n","prompt = tokenizer.apply_chat_template(\n","    messages, tokenize=False, add_generation_prompt=True\n",")\n","\n","outputs = pipe(prompt,truncation=True, do_sample=True)\n","\n","display(\n","    Markdown(\n","            outputs[0][\"generated_text\"].split(\n","                \"<|start_header_id|>assistant<|end_header_id|>\"\n","            )[1]\n","        )\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["### Boolean Face"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:43:09.070741Z","iopub.status.busy":"2024-10-26T00:43:09.070336Z","iopub.status.idle":"2024-10-26T00:43:09.10683Z","shell.execute_reply":"2024-10-26T00:43:09.105832Z","shell.execute_reply.started":"2024-10-26T00:43:09.070698Z"},"trusted":true},"outputs":[],"source":["def boolean_solver(input_data):\n","    # Convert JSON input to numpy array for easier processing\n","    grids = [np.array(grid) for grid in input_data]\n","    \n","    # We assume the operation is \"logical OR\" across all grid inputs\n","    output_grid = np.bitwise_or.reduce(grids)\n","    \n","    return output_grid.tolist()\n","\n","\n","class BooleanOperator(Enum):\n","    AND = \"and\"\n","    OR = \"or\"\n","    XOR = \"xor\"\n","    NAND = \"nand\"\n","    NOR = \"nor\"\n","    NOT = \"not\"\n","    XNOR = \"xnor\"\n","    \n","@dataclass\n","class TruthTableEntry:\n","    input_states: Tuple[np.ndarray, ...]\n","    output_state: np.ndarray\n","    operator: BooleanOperator\n","    score: float\n","    description: str = \"\"\n","\n","class TruthTableAnalyzer:\n","    \"\"\"Analyzes 2x2 matrix patterns using truth tables and cellular automata principles\"\"\"\n","    \n","    def __init__(self, logging_level=logging.INFO):\n","        self.setup_logging(logging_level)\n","        self.truth_table_cache = {}\n","        self.score_history = defaultdict(list)\n","        self.best_rules = {}\n","        \n","    def setup_logging(self, level):\n","        \"\"\"Configure detailed logging for analysis\"\"\"\n","        logging.basicConfig(\n","            level=level,\n","            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n","            handlers=[\n","                logging.FileHandler('boolean_analysis.log'),\n","                logging.StreamHandler()\n","            ]\n","        )\n","        self.logger = logging.getLogger('BooleanAnalyzer')\n","        \n","    def analyze_2x2_pattern(self, input_grid: np.ndarray, output_grid: np.ndarray) -> TruthTableEntry:\n","        \"\"\"Analyze a 2x2 pattern and generate truth table\"\"\"\n","        self.logger.info(f\"\\nAnalyzing 2x2 pattern:\\nInput:\\n{input_grid}\\nOutput:\\n{output_grid}\")\n","        \n","        # Convert to boolean\n","        input_bool = input_grid.astype(bool)\n","        output_bool = output_grid.astype(bool)\n","        \n","        # Generate cache key\n","        cache_key = self._generate_cache_key(input_bool, output_bool)\n","        \n","        if cache_key in self.truth_table_cache:\n","            self.logger.info(\"Found pattern in cache\")\n","            return self.truth_table_cache[cache_key]\n","        \n","        # Try all boolean operators and score results\n","        best_entry = self._find_best_operator(input_bool, output_bool)\n","        \n","        # Cache result\n","        self.truth_table_cache[cache_key] = best_entry\n","        \n","        return best_entry\n","    \n","    def _generate_cache_key(self, input_bool: np.ndarray, output_bool: np.ndarray) -> str:\n","        \"\"\"Generate unique key for caching\"\"\"\n","        return f\"{input_bool.tobytes()}-{output_bool.tobytes()}\"\n","    \n","    def _find_best_operator(self, input_bool: np.ndarray, output_bool: np.ndarray) -> TruthTableEntry:\n","        \"\"\"Try different boolean operators and find best match\"\"\"\n","        best_score = 0.0\n","        best_entry = None\n","        \n","        for operator in BooleanOperator:\n","            result = self._apply_operator(input_bool, operator)\n","            score = self._calculate_match_score(result, output_bool)\n","            \n","            self.logger.debug(f\"Operator {operator.value}:\")\n","            self.logger.debug(f\"Result:\\n{result}\")\n","            self.logger.debug(f\"Score: {score}\")\n","            \n","            if score > best_score:\n","                best_score = score\n","                best_entry = TruthTableEntry(\n","                    input_states=(input_bool,),\n","                    output_state=result,\n","                    operator=operator,\n","                    score=score\n","                )\n","                \n","        self.logger.info(f\"Best operator: {best_entry.operator.value} with score {best_entry.score}\")\n","        return best_entry\n","    \n","    def _apply_operator(self, grid: np.ndarray, operator: BooleanOperator) -> np.ndarray:\n","        \"\"\"Apply boolean operator to grid\"\"\"\n","        if operator == BooleanOperator.NOT:\n","            return ~grid\n","        elif operator == BooleanOperator.AND:\n","            return grid & np.roll(grid, 1, axis=0)\n","        elif operator == BooleanOperator.OR:\n","            return grid | np.roll(grid, 1, axis=0)\n","        elif operator == BooleanOperator.XOR:\n","            return grid ^ np.roll(grid, 1, axis=0)\n","        elif operator == BooleanOperator.NAND:\n","            return ~(grid & np.roll(grid, 1, axis=0))\n","        elif operator == BooleanOperator.NOR:\n","            return ~(grid | np.roll(grid, 1, axis=0))\n","        elif operator == BooleanOperator.XNOR:\n","            return ~(grid ^ np.roll(grid, 1, axis=0))\n","            \n","    def _calculate_match_score(self, result: np.ndarray, target: np.ndarray) -> float:\n","        \"\"\"Calculate match score between result and target\"\"\"\n","        return np.mean(result == target)\n","\n","class BooleanAlgebraFace:\n","    \"\"\"Main Boolean Algebra face implementation\"\"\"\n","    \n","    def __init__(self, llm_pipeline=None):\n","        self.analyzer = TruthTableAnalyzer()\n","        self.llm_pipeline = llm_pipeline\n","        self.learning_rate = 0.1\n","        self.min_confidence = 0.6\n","        \n","    def analyze_task(self, task: Dict) -> Tuple[List[List[int]], float]:\n","        \"\"\"Analyze task and return prediction with confidence score\"\"\"\n","        input_grids = [np.array(pair['input']) for pair in task['train']]\n","        output_grids = [np.array(pair['output']) for pair in task['train']]\n","        \n","        # Analyze patterns starting with 2x2 windows\n","        patterns = self._extract_2x2_patterns(input_grids[0], output_grids[0])\n","        \n","        # Get truth table analysis for each pattern\n","        truth_tables = []\n","        for in_pattern, out_pattern in patterns:\n","            entry = self.analyzer.analyze_2x2_pattern(in_pattern, out_pattern)\n","            truth_tables.append(entry)\n","            \n","        # Generate description using LLM if available\n","        if self.llm_pipeline and any(entry.score > self.min_confidence for entry in truth_tables):\n","            best_entry = max(truth_tables, key=lambda x: x.score)\n","            description = self._generate_llm_description(best_entry)\n","            best_entry.description = description\n","            \n","        # Apply learned patterns to test input\n","        test_input = np.array(task['test']['input'])\n","        prediction, confidence = self._apply_patterns(test_input, truth_tables)\n","        \n","        return prediction.tolist(), confidence\n","    \n","    def _extract_2x2_patterns(self, input_grid: np.ndarray, output_grid: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n","        \"\"\"Extract all 2x2 patterns from input/output grids\"\"\"\n","        patterns = []\n","        rows, cols = input_grid.shape\n","        \n","        for i in range(rows-1):\n","            for j in range(cols-1):\n","                in_pattern = input_grid[i:i+2, j:j+2]\n","                out_pattern = output_grid[i:i+2, j:j+2]\n","                patterns.append((in_pattern, out_pattern))\n","                \n","        return patterns\n","    \n","    def _generate_llm_description(self, entry: TruthTableEntry) -> str:\n","        \"\"\"Generate natural language description of boolean pattern\"\"\"\n","        prompt = f\"\"\"\n","        Describe this boolean pattern transformation:\n","        Input matrix:\n","        {entry.input_states[0]}\n","        \n","        Operation applied: {entry.operator.value}\n","        \n","        Output matrix:\n","        {entry.output_state}\n","        \n","        Score: {entry.score}\n","        \n","        Describe the pattern in natural language:\n","        \"\"\"\n","        \n","        response = self.llm_pipeline(prompt, max_length=100)[0]['generated_text']\n","        return response.strip()\n","    \n","    def _apply_patterns(self, test_input: np.ndarray, truth_tables: List[TruthTableEntry]) -> Tuple[np.ndarray, float]:\n","        \"\"\"Apply learned patterns to test input\"\"\"\n","        # Start with highest scoring patterns\n","        truth_tables.sort(key=lambda x: x.score, reverse=True)\n","        \n","        # Initialize output grid\n","        output = np.zeros_like(test_input)\n","        total_confidence = 0.0\n","        \n","        # Apply each pattern where confidence is high enough\n","        for entry in truth_tables:\n","            if entry.score > self.min_confidence:\n","                transformed = self._apply_operator(test_input, entry.operator)\n","                output = np.where(entry.score > total_confidence, transformed, output)\n","                total_confidence = max(total_confidence, entry.score)\n","                \n","        return output, total_confidence\n","\n","    def update_learning(self, task_id: str, prediction: List[List[int]], actual: List[List[int]], score: float):\n","        \"\"\"Update learning based on feedback\"\"\"\n","        self.analyzer.score_history[task_id].append(score)\n","        \n","        # Adjust learning rate based on historical performance\n","        if len(self.analyzer.score_history[task_id]) > 1:\n","            prev_score = self.analyzer.score_history[task_id][-2]\n","            if score > prev_score:\n","                self.learning_rate *= 1.1\n","            else:\n","                self.learning_rate *= 0.9\n","                \n","        self.analyzer.logger.info(f\"Task {task_id} - Score: {score}, Learning rate: {self.learning_rate}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Usage"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T00:45:43.06936Z","iopub.status.busy":"2024-10-26T00:45:43.068717Z","iopub.status.idle":"2024-10-26T00:45:43.085135Z","shell.execute_reply":"2024-10-26T00:45:43.084018Z","shell.execute_reply.started":"2024-10-26T00:45:43.069317Z"},"trusted":true},"outputs":[],"source":["class ARCProcessor:\n","    def __init__(self, boolean_face=None, llm_pipeline=None):\n","        self.boolean_face = boolean_face or BooleanAlgebraFace(llm_pipeline)\n","        \n","    def process_tasks(self, tasks: Dict) -> Dict:\n","        results = {}\n","        for task_id, task_data in tasks.items():\n","            print(f\"Processing task {task_id}\")\n","            predictions = self.process_single_task(task_data)\n","            results[task_id] = predictions\n","        return results\n","    \n","    def process_single_task(self, task_data: Dict) -> List[Dict]:\n","        # Handle both single and multiple test inputs\n","        test_inputs = task_data.get('test', {})\n","        if isinstance(test_inputs, dict):\n","            test_inputs = [test_inputs]\n","            \n","        predictions = []\n","        for test_input in test_inputs:\n","            task = {\n","                'train': task_data['train'],\n","                'test': test_input\n","            }\n","            pred_grid, confidence = self.boolean_face.analyze_task(task)\n","            \n","            # Create two slightly different attempts\n","            attempt_1 = pred_grid\n","            attempt_2 = self._create_alternative_attempt(pred_grid)\n","            \n","            prediction = {\n","                'attempt_1': attempt_1,\n","                'attempt_2': attempt_2\n","            }\n","            predictions.append(prediction)\n","            \n","        return predictions\n","    \n","    def _create_alternative_attempt(self, pred_grid: List[List[int]]) -> List[List[int]]:\n","        \"\"\"Create a slightly modified version of the prediction for the second attempt\"\"\"\n","        grid = np.array(pred_grid)\n","        if grid.size > 0:\n","            # Randomly modify one cell for variety\n","            i, j = np.random.randint(0, grid.shape[0]), np.random.randint(0, grid.shape[1])\n","            grid[i, j] = 1 - grid[i, j]  # Flip 0 to 1 or 1 to 0\n","        return grid.tolist()\n","    \n","    def save_submission(self, results: Dict, output_path: str = 'submission.json'):\n","        with open(output_path, 'w') as f:\n","            json.dump(results, f, indent=2)\n","            \n","def run_arc_processor(test_file_path: str, llm_pipeline=None):\n","    # Load tasks\n","    with open(test_file_path, 'r') as f:\n","        tasks = json.load(f)\n","    \n","    # Initialize components\n","    print(\"Initializing Boolean Algebra face...\")\n","    boolean_face = BooleanAlgebraFace(llm_pipeline)\n","    \n","    # Create processor\n","    print(\"Creating processor...\")\n","    processor = ARCProcessor(boolean_face, llm_pipeline)\n","    \n","    # Process tasks\n","    print(\"Processing tasks...\")\n","    results = processor.process_tasks(tasks)\n","    \n","    # Save submission\n","    processor.save_submission(results)\n","    return results"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8951125,"sourceId":67357,"sourceType":"competition"},{"datasetId":5943062,"sourceId":9714901,"sourceType":"datasetVersion"},{"modelId":121027,"modelInstanceId":100936,"sourceId":120005,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"McBride-ARC-2024-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
